{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ORVPadj7rKq",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "<b>Group 58</b>\n",
        "* <b> Student 1 </b> : Luc Reinink, 1068948\n",
        "* <b> Student 2 </b> : Gerrit Merz, 1553410\n",
        "\n",
        "**Reading material**\n",
        "* [1] Mikolov, Tomas, et al. \"[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\" Advances in neural information processing systems. 2013. \n",
        "\n",
        "<b><font color='red'>NOTE</font></b> When submitting your notebook, please make sure that the training history of your model is visible in the output. This means that you should **NOT** clean your output cells of the notebook. Make sure that your notebook runs without errors in linear order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rQwLyiMFu_1",
        "colab_type": "text"
      },
      "source": [
        "# Question 1 - Keras implementation (10 pt)\n",
        "\n",
        "### Word embeddings\n",
        "Build word embeddings with a Keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training. Use a window size of 2 to train the embeddings (`window_size` in the jupyter notebook). \n",
        "\n",
        "1. Build word embeddings of length 50, 150 and 300 using the Skipgram model\n",
        "2. Build word embeddings of length 50, 150 and 300 using CBOW model\n",
        "3. Analyze the different word embeddings:\n",
        "    - Implement your own function to perform the analogy task (see [1] for concrete examples). Use the same distance metric as in the paper. Do not use existing libraries for this task such as Gensim. \n",
        "Your function should be able to answer whether an analogy like: \"a king is to a queen as a man is to a woman\" ($e_{king} - e_{queen} + e_{woman} \\approx e_{man}$) is true. $e_{x}$ denotes the embedding of word $x$. We want to find the word $p$ in the vocabulary, where the embedding of $p$ ($e_p$) is the closest to the predicted embedding (i.e. result of the formula). Then, we can check if $p$ is the same word as the true word $t$.\n",
        "    - Give at least 5 different  examples of analogies.\n",
        "    - Compare the performance on the analogy tasks between the word embeddings and briefly discuss your results.\n",
        "\n",
        "4. Discuss:\n",
        "  - Given the same number of sentences as input, CBOW and Skipgram arrange the data into different number of training samples. Which one has more and why?\n",
        "\n",
        "\n",
        "<b>HINT</b> See practical 3.1 for some helpful code to start this assignment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctoyAoX1AI6T",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5VOelR7BYQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCnATRPgBZEd",
        "colab_type": "code",
        "outputId": "67600e61-b874-429f-d5c7-743327ba9114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Reshape, Lambda\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# other helpful libraries\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.neighbors import NearestNeighbors as nn\n",
        "from matplotlib import pylab\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBNCPtOoBbB1",
        "colab_type": "code",
        "outputId": "9761af15-d6d6-4086-f04c-4d530014f069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(tf.__version__) #  check what version of TF is imported"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCd0zUO1AKjY",
        "colab_type": "text"
      },
      "source": [
        "### Import file\n",
        "\n",
        "If you use Google Colab, you need to mount your Google Drive to the notebook when you want to use files that are located in your Google Drive. Paste the authorization code, from the new tab page that opens automatically when running the cell, in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdjNeehKBd-a",
        "colab_type": "code",
        "outputId": "e1aff8fc-0998-4df3-dfed-4073d140c873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        " from google.colab import drive\n",
        " drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjIVBt8YGUaO",
        "colab_type": "text"
      },
      "source": [
        "Navigate to the folder in which `alice.txt` is located. Make sure to start path with '/content/drive/My Drive/' if you want to load the file from your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS0-uINUBfic",
        "colab_type": "code",
        "outputId": "ff5ce350-eef7-419a-f102-f657c13fc32e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd '/content/drive/My Drive/2IMM10 Deep Learning/Assignments/Assignment 1'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/2IMM10 Deep Learning/Assignments/Assignment 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye1jsuImBSOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cd '/content/drive/My Drive/Deep Learning'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz8Z5gCDBhSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = 'Resources/alice.txt'\n",
        "# file_name = 'alice.txt'\n",
        "corpus = open(file_name).readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkbk32wHANnD",
        "colab_type": "text"
      },
      "source": [
        "### Data preprocessing\n",
        "\n",
        "See Practical 3.1 for an explanation of the preprocessing steps done below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37PyOHq2BkY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removes sentences with fewer than 3 words\n",
        "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
        "\n",
        "# remove punctuation in text and fit tokenizer on entire corpus\n",
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "# convert text to sequence of integer values\n",
        "corpus = tokenizer.texts_to_sequences(corpus)\n",
        "n_samples = sum(len(s) for s in corpus) # total number of words in the corpus\n",
        "V = len(tokenizer.word_index) + 1 # total number of unique words in the corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILdA_IimBlte",
        "colab_type": "code",
        "outputId": "93a0ea8f-029c-4a07-c9d8-0392c7e4eecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "n_samples, V"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27165, 2557)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRbpue0WBms6",
        "colab_type": "code",
        "outputId": "62ea4407-ec0e-4baf-ab9c-440ef69835fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# example of how word to integer mapping looks like in the tokenizer\n",
        "print(list((tokenizer.word_index.items()))[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 1), ('and', 2), ('to', 3), ('a', 4), ('it', 5)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er86VxH9BqI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "window_size = 2\n",
        "window_size_corpus = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0sU1JORATvX",
        "colab_type": "text"
      },
      "source": [
        "## Task 1.1 - Skipgram\n",
        "Build word embeddings of length 50, 150 and 300 using the Skipgram model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsPfyhXWbqts",
        "colab_type": "text"
      },
      "source": [
        "### Explanation\n",
        "The sections below motivate some of the choices that were made.\n",
        "\n",
        "#### 1-2 Preparing data for Skipgram\n",
        "While preparing data for the Skipgram, we take all sentences, and loop over all words of the sentence where each word is an input word. For each input word, we take two words before it, and two words after it to create one-hot encoded versions of them as target words. Hence, each word of the corpus has four target words, except for some words at the edges of sentences. \n",
        "\n",
        "#### 3 Creating Skipgram architecture\n",
        "As described in the paper, we first add an `Embedding` layer with `input_dim=V`, so that every word has an embedding vector of the provided `embed_length`. Next, we add a `Reshape` to connect the embedding to a `Dense` layer with the softmax activation. We use softmax because this is the closest to the activation in the paper. We use Glorot uniform initialisers where possible, since this initialiser finds a good variance for the distribution from which the parameters are drawn. This often results in faster learning.\n",
        "\n",
        "#### 4-5 Training\n",
        "We train the data without evaluation set, since the goal of this model is not to find words surrounding the input word, but to extract a good embedding layer. We create three models with embedding lengths of 50, 150 and 300.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WxKD6_9-uXz",
        "colab_type": "code",
        "outputId": "6c5688ee-a4a4-4c6b-af6b-0b5a4df1b6d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 1. prepare data for skipgram\n",
        "def generate_skipgram(corpus, window_size, V):\n",
        "    maxlen = window_size*2\n",
        "    all_in = []\n",
        "    all_out = []\n",
        "    for words in corpus:\n",
        "        L = len(words)\n",
        "        for index, word in enumerate(words):\n",
        "            p = index - window_size\n",
        "            n = index + window_size + 1\n",
        "                    \n",
        "            in_words = []\n",
        "            labels = []\n",
        "            for i in range(p, n):\n",
        "                if i != index and 0 <= i < L:\n",
        "                    # Add the input word\n",
        "                    #in_words.append(word)\n",
        "                    all_in.append(word)\n",
        "                    # Add one-hot of the context words\n",
        "                    all_out.append(to_categorical(words[i], V))\n",
        "                                      \n",
        "    return (np.array(all_in),np.array(all_out))\n",
        "\n",
        "# 2. Create training data\n",
        "x , y = generate_skipgram(corpus,window_size,V)\n",
        "\n",
        "# 3. Create Skipgram architecture\n",
        "def create_skipgram_model(V, window_size, embed_length):\n",
        "  skipgram = Sequential(name=\"SKIPGRAM_\" + str(embed_length))\n",
        "  skipgram.add(Embedding(input_dim=V, output_dim=embed_length, embeddings_initializer='glorot_uniform', input_length=1))\n",
        "  skipgram.add(Reshape((embed_length, )))\n",
        "  skipgram.add(Dense(V, kernel_initializer='glorot_uniform', activation='softmax'))\n",
        "  skipgram.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
        "  return skipgram\n",
        "\n",
        "#4 . Train skipgram model\n",
        "def train_skipgram_model(skipgram, epochs):\n",
        "  skipgram.fit(x, y, batch_size=64, epochs=epochs, verbose = 1)\n",
        "  return skipgram\n",
        "\n",
        "# 5. Save embeddings for vectors of length 50, 150 and 300 using skipgram model.\n",
        "embed_lengths = [50, 150, 300]\n",
        "epochs = 10\n",
        "skipgram_models = []\n",
        "\n",
        "for embed_length in embed_lengths:\n",
        "    skipgram = create_skipgram_model(V, window_size, embed_length)\n",
        "    skipgram = train_skipgram_model(skipgram, epochs)\n",
        "    skipgram_models.append(skipgram)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8461\n",
            "Epoch 2/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8453\n",
            "Epoch 3/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8445\n",
            "Epoch 4/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8436\n",
            "Epoch 5/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8428\n",
            "Epoch 6/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8419\n",
            "Epoch 7/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8411\n",
            "Epoch 8/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8402\n",
            "Epoch 9/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8394\n",
            "Epoch 10/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8386\n",
            "Epoch 1/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8461\n",
            "Epoch 2/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8452\n",
            "Epoch 3/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8444\n",
            "Epoch 4/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8435\n",
            "Epoch 5/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8427\n",
            "Epoch 6/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8418\n",
            "Epoch 7/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8409\n",
            "Epoch 8/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8401\n",
            "Epoch 9/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8392\n",
            "Epoch 10/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8383\n",
            "Epoch 1/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8460\n",
            "Epoch 2/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8451\n",
            "Epoch 3/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8442\n",
            "Epoch 4/10\n",
            "1478/1478 [==============================] - 8s 5ms/step - loss: 7.8433\n",
            "Epoch 5/10\n",
            "1478/1478 [==============================] - 8s 6ms/step - loss: 7.8424\n",
            "Epoch 6/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8415\n",
            "Epoch 7/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8406\n",
            "Epoch 8/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8397\n",
            "Epoch 9/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8388\n",
            "Epoch 10/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4z9Lt6pAZEw",
        "colab_type": "text"
      },
      "source": [
        "## Task 1.2 - CBOW\n",
        "\n",
        "Build word embeddings of length 50, 150 and 300 using CBOW model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP16dPoEwaLK",
        "colab_type": "text"
      },
      "source": [
        "### Explanation\n",
        "The sections below motivate some of the choices that were made.\n",
        "\n",
        "#### 1-2 Preparing data for CBOW\n",
        "While preparing data for the CBOW model, we take all sentences, and loop over all words of the sentence, where each word is a target word. For each target word, we take two words before it, and two words after it to create the context. If one or two words before or after the target word don't exist, we replace it by a 0 to mark it as an unknown value. After the loops, we arrange the words so that the order is maintained. However, this is not necessary for CBOW since word vectors are averaged. This results in word order not making any difference. We did it to make it easier to check if the generated pairs are correct.\n",
        "\n",
        "#### 3 Creating CBOW architecture\n",
        "As described in the paper, we first add an `Embedding` layer with `input_dim=V`, so that every word has an embedding vector of the provided `embed_length`. Next, we add a `Lambda` layer to average the context vectors, as described in the paper. For the last layer, we add a `Dense` layer with the softmax activation because this is the closest to the activation in the paper. We use Glorot uniform initialisers where possible, since this initialiser finds a good variance for the distribution from which the parameters are drawn. This often results in faster learning.\n",
        "\n",
        "#### 4-5 Training\n",
        "We train the data without evaluation set, since the goal of this model is not to classify which word fits in the context, but to extract a good embedding layer. We create three models with embedding lengths of 50, 150 and 300.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMhI_sWFTXDW",
        "colab_type": "code",
        "outputId": "751fc4fb-860b-4f7c-e1d6-3796dda228ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 1. Prepare data for CBOW.\n",
        "def generate_data_cbow(corpus, window_size, V):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for sentence in corpus:\n",
        "        for i in range(len(sentence)):\n",
        "            context_before = []\n",
        "            context_after = []\n",
        "            for j in range(1, window_size + 1):\n",
        "                if (i - j >= 0):\n",
        "                    context_before.append(sentence[i - j])\n",
        "                else:\n",
        "                    context_before.append(0)\n",
        "                if (i + j < len(sentence)):\n",
        "                    context_after.append(sentence[i + j])\n",
        "                else:\n",
        "                    context_after.append(0)\n",
        "\n",
        "            context_before.reverse()\n",
        "            context = context_before\n",
        "            context.extend(context_after)\n",
        "            target = sentence[i]\n",
        "            X.append(context)\n",
        "            y.append(to_categorical(target, V))\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 2. Create training data.\n",
        "X, y = generate_data_cbow(corpus, window_size, V)\n",
        "\n",
        "# 3. Create CBOW architecture\n",
        "def create_cbow_model(V, window_size, embed_length):\n",
        "    cbow = Sequential(name=\"CBOW_\" + str(embed_length))\n",
        "    cbow.add(Embedding(input_dim=V, output_dim=embed_length, \n",
        "                       embeddings_initializer=\"glorot_uniform\", \n",
        "                       input_length=window_size*2))\n",
        "    # Average the context word into a single vector.\n",
        "    cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_length,)))\n",
        "    cbow.add(Dense(V, activation=\"softmax\", \n",
        "                   kernel_initializer=\"glorot_uniform\"))\n",
        "    cbow.compile(optimizer=\"adadelta\", loss=\"categorical_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "    return cbow\n",
        "\n",
        "# 4. Train CBOW model.\n",
        "def train_cbow_model(cbow, epochs):\n",
        "    cbow.fit(X, y, batch_size=64, epochs=epochs)\n",
        "    return cbow\n",
        "\n",
        "# 5. Save embeddings for vectors of length 50, 150 and 300 using CBOW model.\n",
        "embed_lengths = [50, 150, 300]\n",
        "epochs = 10\n",
        "cbow_models = []\n",
        "\n",
        "for embed_length in embed_lengths:\n",
        "    cbow = create_cbow_model(V, window_size, embed_length)\n",
        "    cbow = train_cbow_model(cbow, epochs)\n",
        "    cbow_models.append(cbow)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8466 - accuracy: 2.5768e-04\n",
            "Epoch 2/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8463 - accuracy: 3.6812e-04\n",
            "Epoch 3/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8461 - accuracy: 8.0987e-04\n",
            "Epoch 4/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8458 - accuracy: 9.9393e-04\n",
            "Epoch 5/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8456 - accuracy: 0.0015\n",
            "Epoch 6/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8454 - accuracy: 0.0021\n",
            "Epoch 7/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8451 - accuracy: 0.0034\n",
            "Epoch 8/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8449 - accuracy: 0.0052\n",
            "Epoch 9/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8447 - accuracy: 0.0073\n",
            "Epoch 10/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8444 - accuracy: 0.0099\n",
            "Epoch 1/10\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 7.8464 - accuracy: 4.4174e-04\n",
            "Epoch 2/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8461 - accuracy: 4.7856e-04\n",
            "Epoch 3/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8459 - accuracy: 5.5218e-04\n",
            "Epoch 4/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8456 - accuracy: 6.9943e-04\n",
            "Epoch 5/10\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 7.8454 - accuracy: 9.5711e-04\n",
            "Epoch 6/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8451 - accuracy: 0.0011\n",
            "Epoch 7/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8449 - accuracy: 0.0015\n",
            "Epoch 8/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8446 - accuracy: 0.0020\n",
            "Epoch 9/10\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 7.8443 - accuracy: 0.0028\n",
            "Epoch 10/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8441 - accuracy: 0.0033\n",
            "Epoch 1/10\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 7.8465 - accuracy: 4.7856e-04\n",
            "Epoch 2/10\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 7.8463 - accuracy: 4.7856e-04\n",
            "Epoch 3/10\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 7.8460 - accuracy: 5.1537e-04\n",
            "Epoch 4/10\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 7.8457 - accuracy: 7.3624e-04\n",
            "Epoch 5/10\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 7.8454 - accuracy: 8.4668e-04\n",
            "Epoch 6/10\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 7.8452 - accuracy: 8.8349e-04\n",
            "Epoch 7/10\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 7.8449 - accuracy: 9.5711e-04\n",
            "Epoch 8/10\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 7.8446 - accuracy: 0.0011\n",
            "Epoch 9/10\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 7.8444 - accuracy: 0.0014\n",
            "Epoch 10/10\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 7.8441 - accuracy: 0.0019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnUaJm2fZ02i",
        "colab_type": "code",
        "outputId": "57b2f91b-2413-40f3-d71c-a32f8c2726d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "pd.DataFrame(cbow_models[0].get_weights()[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.003160</td>\n",
              "      <td>-0.044598</td>\n",
              "      <td>-0.022691</td>\n",
              "      <td>-0.027301</td>\n",
              "      <td>-0.045028</td>\n",
              "      <td>0.033939</td>\n",
              "      <td>-0.029093</td>\n",
              "      <td>0.036648</td>\n",
              "      <td>0.045196</td>\n",
              "      <td>0.016631</td>\n",
              "      <td>0.010622</td>\n",
              "      <td>0.003019</td>\n",
              "      <td>-0.026558</td>\n",
              "      <td>-0.026050</td>\n",
              "      <td>-0.020083</td>\n",
              "      <td>-0.018497</td>\n",
              "      <td>0.033937</td>\n",
              "      <td>-0.010353</td>\n",
              "      <td>-0.042910</td>\n",
              "      <td>0.010363</td>\n",
              "      <td>0.007339</td>\n",
              "      <td>0.035711</td>\n",
              "      <td>0.002487</td>\n",
              "      <td>0.027277</td>\n",
              "      <td>-0.044572</td>\n",
              "      <td>0.011258</td>\n",
              "      <td>-0.043617</td>\n",
              "      <td>0.021661</td>\n",
              "      <td>-0.026586</td>\n",
              "      <td>-0.032172</td>\n",
              "      <td>-0.009612</td>\n",
              "      <td>-0.042703</td>\n",
              "      <td>0.024925</td>\n",
              "      <td>0.011072</td>\n",
              "      <td>0.001941</td>\n",
              "      <td>-0.022489</td>\n",
              "      <td>-0.035658</td>\n",
              "      <td>-0.008519</td>\n",
              "      <td>-0.009015</td>\n",
              "      <td>0.022527</td>\n",
              "      <td>-0.029850</td>\n",
              "      <td>0.034572</td>\n",
              "      <td>0.018652</td>\n",
              "      <td>-0.002302</td>\n",
              "      <td>0.008422</td>\n",
              "      <td>0.043052</td>\n",
              "      <td>0.030261</td>\n",
              "      <td>-0.038872</td>\n",
              "      <td>-0.037510</td>\n",
              "      <td>-0.029392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.021964</td>\n",
              "      <td>0.025391</td>\n",
              "      <td>-0.036893</td>\n",
              "      <td>0.041336</td>\n",
              "      <td>-0.012319</td>\n",
              "      <td>-0.027251</td>\n",
              "      <td>-0.008523</td>\n",
              "      <td>-0.030166</td>\n",
              "      <td>-0.020421</td>\n",
              "      <td>0.024410</td>\n",
              "      <td>-0.017594</td>\n",
              "      <td>0.027649</td>\n",
              "      <td>0.011513</td>\n",
              "      <td>-0.012193</td>\n",
              "      <td>0.045672</td>\n",
              "      <td>-0.004220</td>\n",
              "      <td>-0.044305</td>\n",
              "      <td>-0.015557</td>\n",
              "      <td>-0.040187</td>\n",
              "      <td>0.039597</td>\n",
              "      <td>-0.017017</td>\n",
              "      <td>-0.042293</td>\n",
              "      <td>-0.040556</td>\n",
              "      <td>0.046749</td>\n",
              "      <td>0.017214</td>\n",
              "      <td>-0.026304</td>\n",
              "      <td>-0.025587</td>\n",
              "      <td>-0.041689</td>\n",
              "      <td>-0.035455</td>\n",
              "      <td>0.020387</td>\n",
              "      <td>0.004394</td>\n",
              "      <td>0.011694</td>\n",
              "      <td>0.036447</td>\n",
              "      <td>-0.006199</td>\n",
              "      <td>-0.047093</td>\n",
              "      <td>-0.017880</td>\n",
              "      <td>-0.030506</td>\n",
              "      <td>-0.019972</td>\n",
              "      <td>-0.018389</td>\n",
              "      <td>0.013909</td>\n",
              "      <td>-0.032367</td>\n",
              "      <td>-0.028685</td>\n",
              "      <td>-0.027665</td>\n",
              "      <td>-0.026521</td>\n",
              "      <td>0.036848</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>-0.000757</td>\n",
              "      <td>0.013274</td>\n",
              "      <td>0.025449</td>\n",
              "      <td>0.001055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.047277</td>\n",
              "      <td>0.033200</td>\n",
              "      <td>0.004926</td>\n",
              "      <td>-0.043496</td>\n",
              "      <td>-0.038368</td>\n",
              "      <td>-0.019265</td>\n",
              "      <td>0.033222</td>\n",
              "      <td>-0.000083</td>\n",
              "      <td>-0.028099</td>\n",
              "      <td>-0.042271</td>\n",
              "      <td>-0.026451</td>\n",
              "      <td>0.009882</td>\n",
              "      <td>0.036982</td>\n",
              "      <td>-0.018226</td>\n",
              "      <td>-0.006882</td>\n",
              "      <td>0.028230</td>\n",
              "      <td>0.028179</td>\n",
              "      <td>-0.036253</td>\n",
              "      <td>0.045479</td>\n",
              "      <td>-0.012998</td>\n",
              "      <td>-0.003839</td>\n",
              "      <td>-0.027244</td>\n",
              "      <td>-0.013604</td>\n",
              "      <td>-0.010748</td>\n",
              "      <td>0.026709</td>\n",
              "      <td>0.034305</td>\n",
              "      <td>0.010073</td>\n",
              "      <td>-0.032167</td>\n",
              "      <td>0.022005</td>\n",
              "      <td>0.007604</td>\n",
              "      <td>0.021992</td>\n",
              "      <td>-0.032735</td>\n",
              "      <td>-0.033295</td>\n",
              "      <td>0.009248</td>\n",
              "      <td>0.042476</td>\n",
              "      <td>0.001713</td>\n",
              "      <td>0.018134</td>\n",
              "      <td>-0.015443</td>\n",
              "      <td>0.026741</td>\n",
              "      <td>0.041811</td>\n",
              "      <td>0.009153</td>\n",
              "      <td>-0.008215</td>\n",
              "      <td>0.010049</td>\n",
              "      <td>0.018712</td>\n",
              "      <td>-0.006159</td>\n",
              "      <td>-0.020338</td>\n",
              "      <td>-0.005121</td>\n",
              "      <td>-0.042298</td>\n",
              "      <td>0.041980</td>\n",
              "      <td>0.018065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.016407</td>\n",
              "      <td>-0.017723</td>\n",
              "      <td>0.031362</td>\n",
              "      <td>0.045387</td>\n",
              "      <td>0.021351</td>\n",
              "      <td>-0.024102</td>\n",
              "      <td>0.034785</td>\n",
              "      <td>0.017627</td>\n",
              "      <td>-0.020827</td>\n",
              "      <td>0.001635</td>\n",
              "      <td>0.023610</td>\n",
              "      <td>0.040219</td>\n",
              "      <td>0.002360</td>\n",
              "      <td>-0.004440</td>\n",
              "      <td>0.015783</td>\n",
              "      <td>-0.015061</td>\n",
              "      <td>0.033655</td>\n",
              "      <td>0.031443</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>-0.005680</td>\n",
              "      <td>0.018744</td>\n",
              "      <td>0.023590</td>\n",
              "      <td>-0.014390</td>\n",
              "      <td>0.038353</td>\n",
              "      <td>0.016115</td>\n",
              "      <td>0.022340</td>\n",
              "      <td>-0.015692</td>\n",
              "      <td>-0.005659</td>\n",
              "      <td>0.047526</td>\n",
              "      <td>-0.001613</td>\n",
              "      <td>-0.006103</td>\n",
              "      <td>-0.035971</td>\n",
              "      <td>-0.033398</td>\n",
              "      <td>0.045693</td>\n",
              "      <td>-0.026787</td>\n",
              "      <td>0.008014</td>\n",
              "      <td>0.046522</td>\n",
              "      <td>-0.010308</td>\n",
              "      <td>-0.030144</td>\n",
              "      <td>0.020764</td>\n",
              "      <td>-0.017119</td>\n",
              "      <td>0.023611</td>\n",
              "      <td>-0.001372</td>\n",
              "      <td>-0.013454</td>\n",
              "      <td>0.029037</td>\n",
              "      <td>-0.000050</td>\n",
              "      <td>0.025433</td>\n",
              "      <td>0.012419</td>\n",
              "      <td>-0.022872</td>\n",
              "      <td>0.029419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.047382</td>\n",
              "      <td>0.011854</td>\n",
              "      <td>-0.025662</td>\n",
              "      <td>-0.005297</td>\n",
              "      <td>0.007176</td>\n",
              "      <td>-0.042802</td>\n",
              "      <td>-0.043173</td>\n",
              "      <td>-0.018175</td>\n",
              "      <td>0.036246</td>\n",
              "      <td>0.035380</td>\n",
              "      <td>-0.003234</td>\n",
              "      <td>-0.024898</td>\n",
              "      <td>-0.008043</td>\n",
              "      <td>-0.015511</td>\n",
              "      <td>-0.013995</td>\n",
              "      <td>0.025899</td>\n",
              "      <td>0.027597</td>\n",
              "      <td>0.015219</td>\n",
              "      <td>-0.016537</td>\n",
              "      <td>0.036126</td>\n",
              "      <td>0.029253</td>\n",
              "      <td>-0.005997</td>\n",
              "      <td>0.034750</td>\n",
              "      <td>0.007414</td>\n",
              "      <td>0.021962</td>\n",
              "      <td>-0.008539</td>\n",
              "      <td>-0.029028</td>\n",
              "      <td>-0.015255</td>\n",
              "      <td>-0.026723</td>\n",
              "      <td>0.003779</td>\n",
              "      <td>0.002824</td>\n",
              "      <td>-0.004126</td>\n",
              "      <td>-0.045323</td>\n",
              "      <td>0.015125</td>\n",
              "      <td>-0.013146</td>\n",
              "      <td>-0.019170</td>\n",
              "      <td>0.039758</td>\n",
              "      <td>-0.019789</td>\n",
              "      <td>-0.046448</td>\n",
              "      <td>-0.030153</td>\n",
              "      <td>0.020372</td>\n",
              "      <td>0.013563</td>\n",
              "      <td>-0.018378</td>\n",
              "      <td>-0.025887</td>\n",
              "      <td>0.023056</td>\n",
              "      <td>0.034963</td>\n",
              "      <td>0.017486</td>\n",
              "      <td>-0.022901</td>\n",
              "      <td>0.017075</td>\n",
              "      <td>-0.017986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2552</th>\n",
              "      <td>0.016058</td>\n",
              "      <td>-0.023422</td>\n",
              "      <td>0.000718</td>\n",
              "      <td>0.016451</td>\n",
              "      <td>0.031005</td>\n",
              "      <td>-0.025403</td>\n",
              "      <td>0.007734</td>\n",
              "      <td>0.018306</td>\n",
              "      <td>0.039828</td>\n",
              "      <td>0.030876</td>\n",
              "      <td>-0.045993</td>\n",
              "      <td>-0.013638</td>\n",
              "      <td>0.027023</td>\n",
              "      <td>0.035126</td>\n",
              "      <td>-0.014713</td>\n",
              "      <td>-0.037673</td>\n",
              "      <td>-0.014547</td>\n",
              "      <td>-0.017329</td>\n",
              "      <td>0.004547</td>\n",
              "      <td>-0.046253</td>\n",
              "      <td>-0.041963</td>\n",
              "      <td>-0.035031</td>\n",
              "      <td>-0.011598</td>\n",
              "      <td>-0.026943</td>\n",
              "      <td>0.013734</td>\n",
              "      <td>-0.037233</td>\n",
              "      <td>0.039462</td>\n",
              "      <td>0.015614</td>\n",
              "      <td>-0.036819</td>\n",
              "      <td>0.003304</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.023142</td>\n",
              "      <td>-0.040668</td>\n",
              "      <td>0.003700</td>\n",
              "      <td>0.031717</td>\n",
              "      <td>0.015668</td>\n",
              "      <td>0.046502</td>\n",
              "      <td>0.040458</td>\n",
              "      <td>0.018650</td>\n",
              "      <td>-0.023837</td>\n",
              "      <td>0.004692</td>\n",
              "      <td>-0.007501</td>\n",
              "      <td>0.040772</td>\n",
              "      <td>0.029929</td>\n",
              "      <td>0.040461</td>\n",
              "      <td>-0.047618</td>\n",
              "      <td>0.012199</td>\n",
              "      <td>0.020311</td>\n",
              "      <td>0.011519</td>\n",
              "      <td>0.028983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2553</th>\n",
              "      <td>0.012133</td>\n",
              "      <td>-0.018549</td>\n",
              "      <td>-0.029472</td>\n",
              "      <td>-0.004021</td>\n",
              "      <td>0.021166</td>\n",
              "      <td>-0.022940</td>\n",
              "      <td>0.001314</td>\n",
              "      <td>0.034995</td>\n",
              "      <td>-0.019117</td>\n",
              "      <td>0.002123</td>\n",
              "      <td>-0.042949</td>\n",
              "      <td>-0.016343</td>\n",
              "      <td>-0.016063</td>\n",
              "      <td>-0.033093</td>\n",
              "      <td>0.008955</td>\n",
              "      <td>-0.026190</td>\n",
              "      <td>0.007561</td>\n",
              "      <td>0.017338</td>\n",
              "      <td>-0.021278</td>\n",
              "      <td>0.027950</td>\n",
              "      <td>-0.039476</td>\n",
              "      <td>-0.015620</td>\n",
              "      <td>0.035698</td>\n",
              "      <td>-0.034358</td>\n",
              "      <td>0.042389</td>\n",
              "      <td>0.039134</td>\n",
              "      <td>0.024436</td>\n",
              "      <td>0.024816</td>\n",
              "      <td>-0.006029</td>\n",
              "      <td>0.044174</td>\n",
              "      <td>0.022035</td>\n",
              "      <td>0.037292</td>\n",
              "      <td>-0.008732</td>\n",
              "      <td>-0.003617</td>\n",
              "      <td>0.011482</td>\n",
              "      <td>0.035882</td>\n",
              "      <td>-0.016076</td>\n",
              "      <td>0.017827</td>\n",
              "      <td>-0.037035</td>\n",
              "      <td>-0.021931</td>\n",
              "      <td>-0.000728</td>\n",
              "      <td>-0.012227</td>\n",
              "      <td>0.036359</td>\n",
              "      <td>0.043644</td>\n",
              "      <td>0.026137</td>\n",
              "      <td>-0.018276</td>\n",
              "      <td>0.010664</td>\n",
              "      <td>0.047744</td>\n",
              "      <td>0.020993</td>\n",
              "      <td>0.018589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2554</th>\n",
              "      <td>-0.044441</td>\n",
              "      <td>0.032632</td>\n",
              "      <td>-0.003166</td>\n",
              "      <td>-0.025483</td>\n",
              "      <td>-0.019613</td>\n",
              "      <td>-0.019278</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>-0.005454</td>\n",
              "      <td>0.047833</td>\n",
              "      <td>0.037355</td>\n",
              "      <td>-0.003038</td>\n",
              "      <td>0.036702</td>\n",
              "      <td>0.028664</td>\n",
              "      <td>0.030424</td>\n",
              "      <td>0.025799</td>\n",
              "      <td>0.016142</td>\n",
              "      <td>-0.032917</td>\n",
              "      <td>0.027476</td>\n",
              "      <td>-0.022885</td>\n",
              "      <td>0.021594</td>\n",
              "      <td>-0.027195</td>\n",
              "      <td>0.026230</td>\n",
              "      <td>-0.047075</td>\n",
              "      <td>-0.039135</td>\n",
              "      <td>-0.040725</td>\n",
              "      <td>0.015439</td>\n",
              "      <td>0.042844</td>\n",
              "      <td>-0.030973</td>\n",
              "      <td>-0.041198</td>\n",
              "      <td>0.031295</td>\n",
              "      <td>0.034244</td>\n",
              "      <td>0.021109</td>\n",
              "      <td>-0.003210</td>\n",
              "      <td>0.025499</td>\n",
              "      <td>0.011775</td>\n",
              "      <td>-0.022886</td>\n",
              "      <td>-0.023223</td>\n",
              "      <td>-0.033364</td>\n",
              "      <td>0.040503</td>\n",
              "      <td>0.014590</td>\n",
              "      <td>-0.022475</td>\n",
              "      <td>-0.014596</td>\n",
              "      <td>-0.005806</td>\n",
              "      <td>-0.016817</td>\n",
              "      <td>-0.005206</td>\n",
              "      <td>-0.026769</td>\n",
              "      <td>0.027993</td>\n",
              "      <td>0.023866</td>\n",
              "      <td>-0.021638</td>\n",
              "      <td>0.035950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2555</th>\n",
              "      <td>0.031610</td>\n",
              "      <td>0.043039</td>\n",
              "      <td>0.004712</td>\n",
              "      <td>0.011941</td>\n",
              "      <td>-0.034621</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>-0.006990</td>\n",
              "      <td>0.008463</td>\n",
              "      <td>-0.044059</td>\n",
              "      <td>0.026455</td>\n",
              "      <td>0.034073</td>\n",
              "      <td>-0.030494</td>\n",
              "      <td>-0.003949</td>\n",
              "      <td>-0.038616</td>\n",
              "      <td>0.024017</td>\n",
              "      <td>-0.028927</td>\n",
              "      <td>0.002829</td>\n",
              "      <td>0.019450</td>\n",
              "      <td>0.005806</td>\n",
              "      <td>0.023589</td>\n",
              "      <td>-0.006409</td>\n",
              "      <td>0.022190</td>\n",
              "      <td>-0.029825</td>\n",
              "      <td>-0.018320</td>\n",
              "      <td>-0.046743</td>\n",
              "      <td>0.012812</td>\n",
              "      <td>-0.000242</td>\n",
              "      <td>0.038028</td>\n",
              "      <td>-0.039503</td>\n",
              "      <td>-0.042036</td>\n",
              "      <td>-0.031100</td>\n",
              "      <td>-0.000142</td>\n",
              "      <td>-0.002833</td>\n",
              "      <td>-0.007873</td>\n",
              "      <td>0.013970</td>\n",
              "      <td>-0.013227</td>\n",
              "      <td>-0.037101</td>\n",
              "      <td>0.007464</td>\n",
              "      <td>0.035066</td>\n",
              "      <td>-0.014960</td>\n",
              "      <td>0.025119</td>\n",
              "      <td>-0.022639</td>\n",
              "      <td>0.026728</td>\n",
              "      <td>0.047484</td>\n",
              "      <td>-0.004632</td>\n",
              "      <td>-0.044966</td>\n",
              "      <td>-0.025029</td>\n",
              "      <td>-0.024690</td>\n",
              "      <td>0.040140</td>\n",
              "      <td>0.013173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2556</th>\n",
              "      <td>0.032673</td>\n",
              "      <td>-0.038118</td>\n",
              "      <td>0.018469</td>\n",
              "      <td>0.024883</td>\n",
              "      <td>-0.022083</td>\n",
              "      <td>-0.020082</td>\n",
              "      <td>-0.032266</td>\n",
              "      <td>0.033764</td>\n",
              "      <td>0.014063</td>\n",
              "      <td>0.043437</td>\n",
              "      <td>-0.027633</td>\n",
              "      <td>-0.041153</td>\n",
              "      <td>-0.038763</td>\n",
              "      <td>-0.037527</td>\n",
              "      <td>-0.001080</td>\n",
              "      <td>0.019402</td>\n",
              "      <td>0.000998</td>\n",
              "      <td>-0.023833</td>\n",
              "      <td>0.009578</td>\n",
              "      <td>-0.006609</td>\n",
              "      <td>-0.046160</td>\n",
              "      <td>-0.002509</td>\n",
              "      <td>-0.033289</td>\n",
              "      <td>0.022567</td>\n",
              "      <td>-0.041034</td>\n",
              "      <td>0.025470</td>\n",
              "      <td>0.017698</td>\n",
              "      <td>0.040680</td>\n",
              "      <td>0.025701</td>\n",
              "      <td>0.044097</td>\n",
              "      <td>0.046352</td>\n",
              "      <td>0.021220</td>\n",
              "      <td>-0.018321</td>\n",
              "      <td>0.023903</td>\n",
              "      <td>0.043978</td>\n",
              "      <td>-0.019984</td>\n",
              "      <td>0.003926</td>\n",
              "      <td>0.045958</td>\n",
              "      <td>-0.022819</td>\n",
              "      <td>-0.033798</td>\n",
              "      <td>0.023233</td>\n",
              "      <td>0.039314</td>\n",
              "      <td>-0.047438</td>\n",
              "      <td>0.030304</td>\n",
              "      <td>0.037988</td>\n",
              "      <td>-0.040666</td>\n",
              "      <td>-0.007238</td>\n",
              "      <td>0.047099</td>\n",
              "      <td>-0.024406</td>\n",
              "      <td>-0.012453</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2557 rows × 50 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        47        48        49\n",
              "0    -0.003160 -0.044598 -0.022691  ... -0.038872 -0.037510 -0.029392\n",
              "1    -0.021964  0.025391 -0.036893  ...  0.013274  0.025449  0.001055\n",
              "2    -0.047277  0.033200  0.004926  ... -0.042298  0.041980  0.018065\n",
              "3     0.016407 -0.017723  0.031362  ...  0.012419 -0.022872  0.029419\n",
              "4     0.047382  0.011854 -0.025662  ... -0.022901  0.017075 -0.017986\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "2552  0.016058 -0.023422  0.000718  ...  0.020311  0.011519  0.028983\n",
              "2553  0.012133 -0.018549 -0.029472  ...  0.047744  0.020993  0.018589\n",
              "2554 -0.044441  0.032632 -0.003166  ...  0.023866 -0.021638  0.035950\n",
              "2555  0.031610  0.043039  0.004712  ... -0.024690  0.040140  0.013173\n",
              "2556  0.032673 -0.038118  0.018469  ...  0.047099 -0.024406 -0.012453\n",
              "\n",
              "[2557 rows x 50 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr2BAI4nAfnG",
        "colab_type": "text"
      },
      "source": [
        "## Task 1.3 - Analogy function\n",
        "\n",
        "Implement your own function to perform the analogy task (see [1] for concrete examples). Use the same distance metric as in [1]. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an analogy like: \"a king is to a queen as a man is to a woman\" ($e_{king} - e_{queen} + e_{woman} \\approx e_{man}$) is true. \n",
        "\n",
        "In a perfect scenario, we would like that this analogy ( $e_{king} - e_{queen} + e_{woman}$) results in the embedding of the word \"man\". However, it does not always result in exactly the same word embedding. The result of the formula is called the expected or the predicted word embedding. In this context, \"man\" is called the true or the actual word $t$. We want to find the word $p$ in the vocabulary, where the embedding of $p$ ($e_p$) is the closest to the predicted embedding (i.e. result of the formula). Then, we can check if $p$ is the same word as the true word $t$.  \n",
        "\n",
        "You have to answer an analogy function using each embedding for both CBOW and Skipgram model. This means that for each analogy we have 6 outputs. Show the true word (with distance similarity value between predicted embedding and true word embedding, i.e. `sim1`) , the predicted word (with distance similarity value between predicted embedding and the embedding of the word in the vocabulary that is closest to this predicted embedding, i.e. `sim2`) and a boolean answer whether the predicted word **exactly** equals the true word. \n",
        "\n",
        "<b>HINT</b>: to visualize the results of the analogy tasks , you can print them in a table. An example is given below.\n",
        "\n",
        "\n",
        "| Analogy task | True word (sim1)  | Predicted word (sim2) | Embedding | Correct?|\n",
        "|------|------|------|------|------|\n",
        "|  queen is to king as woman is to ?\t | man (sim1) | predictd_word(sim2) | SG_50 | True / False|\n",
        "\n",
        "* Give at least 5 different  examples of analogies.\n",
        "* Compare the performance on the analogy s between the word embeddings and briefly discuss your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af9XAEFR6_YU",
        "colab_type": "text"
      },
      "source": [
        "### Explanation\n",
        "We use the cosine similarity to convert the matrix into a similarity vector $v$, where each value $v_i$ denotes the similarity of word $i \\in V$ to the provided `predicted_embedding`. Hence, we return the word with the index of the largest value in $v$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAXD1IeLsCpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding(model):\n",
        "    weights = model.get_weights()\n",
        "    return weights[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKejJIC9PVhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embed(word, embedding):\n",
        "    # Get the index of the word from the tokenizer, \n",
        "    # i.e. convert the string to it's corresponding integer in the vocabulary.\n",
        "    int_word = tokenizer.texts_to_sequences([word])[0]\n",
        "    # Get the one-hot encoding of the word.\n",
        "    bin_word = to_categorical(int_word, V)\n",
        "    return np.dot(bin_word, embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2JV1ZD3xP8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_most_similar(predicted_embedding, embedding_matrix, reverse_word_map):\n",
        "    similarity_vector = cosine_similarity(embedding_matrix, predicted_embedding)\n",
        "    max_sim = similarity_vector[0][0]\n",
        "    max_index = 0\n",
        "    for i in range(1, len(similarity_vector)):\n",
        "        sim = similarity_vector[i][0]\n",
        "        if sim > max_sim:\n",
        "            max_sim = sim\n",
        "            max_index = i\n",
        "    \n",
        "    # Return tuple with (most_similar_word, similarity_value)\n",
        "    return (reverse_word_map.get(max_index), max_sim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7-9m5FkFvp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "all_models = cbow_models + skipgram_models\n",
        "embeddings = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_dQpFqapT8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for model in all_models:\n",
        "    embedding = get_embedding(model)\n",
        "    true_word = \"man\"\n",
        "\n",
        "    # Get the actual embedding of the word.\n",
        "    true_embedding = embed(true_word, embedding)\n",
        "    # Get the predicted embedding of the word.\n",
        "    predicted_embedding = embed(\"king\", embedding) - \\\n",
        "        embed(\"queen\", embedding) + embed(\"woman\", embedding)\n",
        "\n",
        "    # Calculate the similarity between the predicted embedding,\n",
        "    # and the true embedding, and save it as a (word, similarity_val) tuple.\n",
        "    sim1 = (true_word, cosine_similarity(predicted_embedding, \n",
        "                                         true_embedding)[0][0])\n",
        "    # Get the most similar word to the predicted_embedding \n",
        "    # in (most_similar_word, similarity_val) tuple.\n",
        "    predicted_word_tuple = get_most_similar(predicted_embedding, \n",
        "                                            embedding, reverse_word_map)\n",
        "    sim2 = predicted_word_tuple\n",
        "    embedding_name = model.name\n",
        "    is_correct = predicted_word_tuple[0] == true_word\n",
        "\n",
        "    # Combine everything for easy DataFrame addition.\n",
        "    embedding_tuple = (\"a king is to a queen as a man is to a woman\", \n",
        "                sim1, sim2, embedding_name, is_correct)\n",
        "\n",
        "    embeddings.append(embedding_tuple)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLDpmjSypUxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for model in all_models:\n",
        "    embedding = get_embedding(model)\n",
        "    true_word = \"queen\"\n",
        "\n",
        "    # Get the actual embedding of the word.\n",
        "    true_embedding = embed(true_word, embedding)\n",
        "    # Get the predicted embedding of the word.\n",
        "    predicted_embedding = embed(\"king\", embedding) - \\\n",
        "        embed(\"man\", embedding) + embed(\"woman\", embedding)\n",
        "\n",
        "    # Calculate the similarity between the predicted embedding,\n",
        "    # and the true embedding, and save it as a (word, similarity_val) tuple.\n",
        "    sim1 = (true_word, cosine_similarity(predicted_embedding, \n",
        "                                         true_embedding)[0][0])\n",
        "    # Get the most similar word to the predicted_embedding \n",
        "    # in (most_similar_word, similarity_val) tuple.\n",
        "    predicted_word_tuple = get_most_similar(predicted_embedding, \n",
        "                                            embedding, reverse_word_map)\n",
        "    sim2 = predicted_word_tuple\n",
        "    embedding_name = model.name\n",
        "    is_correct = predicted_word_tuple[0] == true_word\n",
        "\n",
        "    # Combine everything for easy DataFrame addition.\n",
        "    embedding_tuple = (\"a king is to a man as a queen is to a woman\", \n",
        "                sim1, sim2, embedding_name, is_correct)\n",
        "\n",
        "    embeddings.append(embedding_tuple)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEhjKxuAIUlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for model in all_models:\n",
        "    embedding = get_embedding(model)\n",
        "    true_word = \"spoke\"\n",
        "\n",
        "    # Get the actual embedding of the word.\n",
        "    true_embedding = embed(true_word, embedding)\n",
        "    # Get the predicted embedding of the word.\n",
        "    predicted_embedding = embed(\"went\", embedding) - \\\n",
        "        embed(\"go\", embedding) + embed(\"speak\", embedding)\n",
        "\n",
        "    # Calculate the similarity between the predicted embedding,\n",
        "    # and the true embedding, and save it as a (word, similarity_val) tuple.\n",
        "    sim1 = (true_word, cosine_similarity(predicted_embedding, \n",
        "                                         true_embedding)[0][0])\n",
        "    # Get the most similar word to the predicted_embedding \n",
        "    # in (most_similar_word, similarity_val) tuple.\n",
        "    predicted_word_tuple = get_most_similar(predicted_embedding, \n",
        "                                            embedding, reverse_word_map)\n",
        "    sim2 = predicted_word_tuple\n",
        "    embedding_name = model.name\n",
        "    is_correct = predicted_word_tuple[0] == true_word\n",
        "\n",
        "    # Combine everything for easy DataFrame addition.\n",
        "    embedding_tuple = (\"went is to go as spoke is to speak\", \n",
        "                sim1, sim2, embedding_name, is_correct)\n",
        "\n",
        "    embeddings.append(embedding_tuple)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMImBE00K6XY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for model in all_models:\n",
        "    embedding = get_embedding(model)\n",
        "    true_word = \"before\"\n",
        "\n",
        "    # Get the actual embedding of the word.\n",
        "    true_embedding = embed(true_word, embedding)\n",
        "    # Get the predicted embedding of the word.\n",
        "    predicted_embedding = embed(\"up\", embedding) - \\\n",
        "        embed(\"down\", embedding) + embed(\"after\", embedding)\n",
        "\n",
        "    # Calculate the similarity between the predicted embedding,\n",
        "    # and the true embedding, and save it as a (word, similarity_val) tuple.\n",
        "    sim1 = (true_word, cosine_similarity(predicted_embedding, \n",
        "                                         true_embedding)[0][0])\n",
        "    # Get the most similar word to the predicted_embedding \n",
        "    # in (most_similar_word, similarity_val) tuple.\n",
        "    predicted_word_tuple = get_most_similar(predicted_embedding, \n",
        "                                            embedding, reverse_word_map)\n",
        "    sim2 = predicted_word_tuple\n",
        "    embedding_name = model.name\n",
        "    is_correct = predicted_word_tuple[0] == true_word\n",
        "\n",
        "    # Combine everything for easy DataFrame addition.\n",
        "    embedding_tuple = (\"up is to down as before is to after\", \n",
        "                sim1, sim2, embedding_name, is_correct)\n",
        "\n",
        "    embeddings.append(embedding_tuple)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvqyWXKrsNrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for model in all_models:\n",
        "    embedding = get_embedding(model)\n",
        "    true_word = \"smallest\"\n",
        "\n",
        "    # Get the actual embedding of the word.\n",
        "    true_embedding = embed(true_word, embedding)\n",
        "    # Get the predicted embedding of the word.\n",
        "    predicted_embedding = embed(\"largest\", embedding) - \\\n",
        "        embed(\"large\", embedding) + embed(\"small\", embedding)\n",
        "\n",
        "    # Calculate the similarity between the predicted embedding,\n",
        "    # and the true embedding, and save it as a (word, similarity_val) tuple.\n",
        "    sim1 = (true_word, cosine_similarity(predicted_embedding, \n",
        "                                         true_embedding)[0][0])\n",
        "    # Get the most similar word to the predicted_embedding \n",
        "    # in (most_similar_word, similarity_val) tuple.\n",
        "    predicted_word_tuple = get_most_similar(predicted_embedding, \n",
        "                                            embedding, reverse_word_map)\n",
        "    sim2 = predicted_word_tuple\n",
        "    embedding_name = model.name\n",
        "    is_correct = predicted_word_tuple[0] == true_word\n",
        "\n",
        "    # Combine everything for easy DataFrame addition.\n",
        "    embedding_tuple = (\"largest is to large as smallest is to small\", \n",
        "                sim1, sim2, embedding_name, is_correct)\n",
        "\n",
        "    embeddings.append(embedding_tuple)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsNdHIPwr74E",
        "colab_type": "text"
      },
      "source": [
        "### Discussion of results\n",
        "The first observation is that none of the results are correct. This was expected due to the fact that the corpus is very small. However, we also observe that the predicted word is always one of the \"added\" words in the vector---not the subtracted word. This is somewhat promising because this means that the models have not learned total nonsense. The true word similarity is almost always very low. This is probably due to the fact that the model did not have a chance to learn it properly, because the word is too rare.\n",
        "\n",
        "We can also observe that the larger embeddings often have a higher similarity value. This is expected, since a larger dimension can fit more information, and therefore, learn more. Finally, we observe that the Skipgram model in general achieves somewhat higher similarity values, which is also expected due to the larger amount of training data (as explained below).\n",
        "\n",
        "The results could probably be improved with more epochs. Overfitting is not a big problem for the models of this assignment, since the models' goals are to learn a compact representation of the text. CBOW could overfit on frequent words, but this is a problem with CBOW in general. The best way to get better results is to use a (much) larger dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXfUkz6n9x70",
        "colab_type": "code",
        "outputId": "54ceaf51-d530-4086-9ad0-409d82205e6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        }
      },
      "source": [
        "df = pd.DataFrame(columns=[\"Analogy Task\", \"True word (sim1)\", \"Predicted word (sim2)\", \"Embedding\", \"Correct?\"])\n",
        "\n",
        "for i in range(len(embeddings)):\n",
        "    df.loc[i] = embeddings[i]\n",
        "\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Analogy Task</th>\n",
              "      <th>True word (sim1)</th>\n",
              "      <th>Predicted word (sim2)</th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Correct?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a king is to a queen as a man is to a woman</td>\n",
              "      <td>(man, -0.13910052)</td>\n",
              "      <td>(king, 0.5858841)</td>\n",
              "      <td>CBOW_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a king is to a queen as a man is to a woman</td>\n",
              "      <td>(man, -0.015250909)</td>\n",
              "      <td>(woman, 0.53653497)</td>\n",
              "      <td>CBOW_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a king is to a queen as a man is to a woman</td>\n",
              "      <td>(man, -0.027084215)</td>\n",
              "      <td>(woman, 0.60925084)</td>\n",
              "      <td>CBOW_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a king is to a queen as a man is to a woman</td>\n",
              "      <td>(man, 0.026587266)</td>\n",
              "      <td>(woman, 0.6127698)</td>\n",
              "      <td>SKIPGRAM_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a king is to a queen as a man is to a woman</td>\n",
              "      <td>(man, -0.0278957)</td>\n",
              "      <td>(king, 0.59490204)</td>\n",
              "      <td>SKIPGRAM_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>a king is to a queen as a man is to a woman</td>\n",
              "      <td>(man, 0.069269456)</td>\n",
              "      <td>(king, 0.66545063)</td>\n",
              "      <td>SKIPGRAM_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>a king is to a man as a queen is to a woman</td>\n",
              "      <td>(queen, 0.054794166)</td>\n",
              "      <td>(king, 0.5376383)</td>\n",
              "      <td>CBOW_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>a king is to a man as a queen is to a woman</td>\n",
              "      <td>(queen, 0.1011336)</td>\n",
              "      <td>(king, 0.5778287)</td>\n",
              "      <td>CBOW_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>a king is to a man as a queen is to a woman</td>\n",
              "      <td>(queen, -0.004370478)</td>\n",
              "      <td>(woman, 0.59991646)</td>\n",
              "      <td>CBOW_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>a king is to a man as a queen is to a woman</td>\n",
              "      <td>(queen, 0.10654198)</td>\n",
              "      <td>(king, 0.6757214)</td>\n",
              "      <td>SKIPGRAM_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>a king is to a man as a queen is to a woman</td>\n",
              "      <td>(queen, 0.06962306)</td>\n",
              "      <td>(king, 0.56698304)</td>\n",
              "      <td>SKIPGRAM_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>a king is to a man as a queen is to a woman</td>\n",
              "      <td>(queen, 0.024168177)</td>\n",
              "      <td>(king, 0.63443255)</td>\n",
              "      <td>SKIPGRAM_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>went is to go as spoke is to speak</td>\n",
              "      <td>(spoke, 0.26007333)</td>\n",
              "      <td>(speak, 0.5951277)</td>\n",
              "      <td>CBOW_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>went is to go as spoke is to speak</td>\n",
              "      <td>(spoke, 0.14067836)</td>\n",
              "      <td>(speak, 0.5654133)</td>\n",
              "      <td>CBOW_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>went is to go as spoke is to speak</td>\n",
              "      <td>(spoke, 0.16097093)</td>\n",
              "      <td>(went, 0.5617652)</td>\n",
              "      <td>CBOW_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>went is to go as spoke is to speak</td>\n",
              "      <td>(spoke, 0.17064776)</td>\n",
              "      <td>(speak, 0.5665556)</td>\n",
              "      <td>SKIPGRAM_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>went is to go as spoke is to speak</td>\n",
              "      <td>(spoke, 0.08834068)</td>\n",
              "      <td>(went, 0.5902064)</td>\n",
              "      <td>SKIPGRAM_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>went is to go as spoke is to speak</td>\n",
              "      <td>(spoke, 0.02630672)</td>\n",
              "      <td>(went, 0.5984255)</td>\n",
              "      <td>SKIPGRAM_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>up is to down as before is to after</td>\n",
              "      <td>(before, 0.1514884)</td>\n",
              "      <td>(up, 0.69736624)</td>\n",
              "      <td>CBOW_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>up is to down as before is to after</td>\n",
              "      <td>(before, 0.12190465)</td>\n",
              "      <td>(after, 0.60252625)</td>\n",
              "      <td>CBOW_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>up is to down as before is to after</td>\n",
              "      <td>(before, -0.12683026)</td>\n",
              "      <td>(after, 0.5527406)</td>\n",
              "      <td>CBOW_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>up is to down as before is to after</td>\n",
              "      <td>(before, 0.3212507)</td>\n",
              "      <td>(after, 0.64318746)</td>\n",
              "      <td>SKIPGRAM_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>up is to down as before is to after</td>\n",
              "      <td>(before, 0.070340104)</td>\n",
              "      <td>(up, 0.62412286)</td>\n",
              "      <td>SKIPGRAM_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>up is to down as before is to after</td>\n",
              "      <td>(before, -0.08199404)</td>\n",
              "      <td>(up, 0.60173583)</td>\n",
              "      <td>SKIPGRAM_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>largest is to large as smallest is to small</td>\n",
              "      <td>(smallest, 0.09757851)</td>\n",
              "      <td>(largest, 0.55662626)</td>\n",
              "      <td>CBOW_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>largest is to large as smallest is to small</td>\n",
              "      <td>(smallest, -0.029537331)</td>\n",
              "      <td>(small, 0.6613456)</td>\n",
              "      <td>CBOW_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>largest is to large as smallest is to small</td>\n",
              "      <td>(smallest, 0.011753817)</td>\n",
              "      <td>(small, 0.6275351)</td>\n",
              "      <td>CBOW_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>largest is to large as smallest is to small</td>\n",
              "      <td>(smallest, -0.020798152)</td>\n",
              "      <td>(largest, 0.5257553)</td>\n",
              "      <td>SKIPGRAM_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>largest is to large as smallest is to small</td>\n",
              "      <td>(smallest, -0.09425397)</td>\n",
              "      <td>(small, 0.59302074)</td>\n",
              "      <td>SKIPGRAM_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>largest is to large as smallest is to small</td>\n",
              "      <td>(smallest, -0.040113684)</td>\n",
              "      <td>(small, 0.587744)</td>\n",
              "      <td>SKIPGRAM_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Analogy Task  ... Correct?\n",
              "0   a king is to a queen as a man is to a woman  ...    False\n",
              "1   a king is to a queen as a man is to a woman  ...    False\n",
              "2   a king is to a queen as a man is to a woman  ...    False\n",
              "3   a king is to a queen as a man is to a woman  ...    False\n",
              "4   a king is to a queen as a man is to a woman  ...    False\n",
              "5   a king is to a queen as a man is to a woman  ...    False\n",
              "6   a king is to a man as a queen is to a woman  ...    False\n",
              "7   a king is to a man as a queen is to a woman  ...    False\n",
              "8   a king is to a man as a queen is to a woman  ...    False\n",
              "9   a king is to a man as a queen is to a woman  ...    False\n",
              "10  a king is to a man as a queen is to a woman  ...    False\n",
              "11  a king is to a man as a queen is to a woman  ...    False\n",
              "12           went is to go as spoke is to speak  ...    False\n",
              "13           went is to go as spoke is to speak  ...    False\n",
              "14           went is to go as spoke is to speak  ...    False\n",
              "15           went is to go as spoke is to speak  ...    False\n",
              "16           went is to go as spoke is to speak  ...    False\n",
              "17           went is to go as spoke is to speak  ...    False\n",
              "18          up is to down as before is to after  ...    False\n",
              "19          up is to down as before is to after  ...    False\n",
              "20          up is to down as before is to after  ...    False\n",
              "21          up is to down as before is to after  ...    False\n",
              "22          up is to down as before is to after  ...    False\n",
              "23          up is to down as before is to after  ...    False\n",
              "24  largest is to large as smallest is to small  ...    False\n",
              "25  largest is to large as smallest is to small  ...    False\n",
              "26  largest is to large as smallest is to small  ...    False\n",
              "27  largest is to large as smallest is to small  ...    False\n",
              "28  largest is to large as smallest is to small  ...    False\n",
              "29  largest is to large as smallest is to small  ...    False\n",
              "\n",
              "[30 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h-OWIF9AsKH",
        "colab_type": "text"
      },
      "source": [
        "## Task 1.4 - Discussion\n",
        "Answer the following question:\n",
        "* Given the same number of sentences as input, CBOW and Skipgram arrange the data into different number of training samples. Which one has more and why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdtXDrbyzPkF",
        "colab_type": "text"
      },
      "source": [
        "### Answer\n",
        "With Skipgram, we create four training samples for each input word. With CBOW, we create only 1 training sample for each input word. Hence, the Skipgram model will have more training samples with the same number of sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOa7EXhD-saI",
        "colab_type": "text"
      },
      "source": [
        "# Question 2 - Peer review (0 pt):\n",
        "Finally, each group member must write a single paragraph outlining their opinion on the work distribution within the group. Did every group member\n",
        "contribute equally? Did you split up tasks in a fair manner, or jointly worked through the exercises. Do you think that some members of your group deserve a different grade from others? You can use the table below to make an overview of how the tasks were divided:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7QLzEjhqAD0",
        "colab_type": "text"
      },
      "source": [
        "__Luc__: Gerrit has been sick for the last couple of days so I will be speaking on his behalf as well. In my opinion, I did some more work than Gerrit. However, this was out of my own motivation, and because of the fact that Gerrit has been sick for a couple of days. Moreover, the tasks we split up were finished on time. I think we deserve an equal grade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWdcoJH__MqP",
        "colab_type": "text"
      },
      "source": [
        "| Student name | Task  |\n",
        "|------|------|\n",
        "|  Luc Reinink  | CBOW implementation, code for generating similarity values and analogy table, analyse results. |\n",
        "| Gerrit Merz  | Skipgram implementation, checking correctness of CBOW implementation. |\n",
        "| Everyone | Try out different analogies to find interesting results. |\n"
      ]
    }
  ]
}