{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment 2.2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uj4T8PEHGbMF"
      },
      "source": [
        "# Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p-gkaM1tCThc"
      },
      "source": [
        "***\n",
        "## Question 2: Triplet networks & one-shot learning (10pt)\n",
        "\n",
        "In practice 4b.4, we train a Siamese network for one-shot learning task on the Omniglot dataset.  In this assignment, we will work on the same data set with the same task but extend it to triplet networks, we will also compare our model performance under different triplet selection method. The assignment contains the following 4 tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZqdQgnI5AuN5"
      },
      "source": [
        "### Import packages and mount data\n",
        "Before everything, we need to import packages and mount data.\n",
        "\n",
        "*HINT: you could use the dataset in practice 4b.4 directly*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8QdDDUEIAuN6",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Input, Conv2D, Lambda, Dense, Flatten, MaxPooling2D, Dropout,Concatenate, BatchNormalization\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NEy5u5WBAuN_",
        "outputId": "4fd745ee-2f4b-4689-e1f0-bb62dd68150c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "PATH = os.path.join(\"drive\", \"My Drive\", \"2IMM10 Deep Learning\", \n",
        "                    \"data_DL_practical\", \"omniglot\")\n",
        "\n",
        "with open(os.path.join(PATH, \"omniglot_train.p\"), \"rb\") as f:\n",
        "    (X_train, c_train) = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(PATH, \"omniglot_test.p\"), \"rb\") as f:\n",
        "    (X_test, c_test) = pickle.load(f)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"\")\n",
        "print(\"training alphabets\")\n",
        "print([key for key in c_train.keys()])\n",
        "print(\"test alphabets:\")\n",
        "print([key for key in c_test.keys()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (964, 20, 105, 105)\n",
            "X_test shape: (659, 20, 105, 105)\n",
            "\n",
            "training alphabets\n",
            "['Braille', 'Anglo-Saxon_Futhorc', 'Tifinagh', 'Grantha', 'Burmese_(Myanmar)', 'Mkhedruli_(Georgian)', 'Latin', 'Ojibwe_(Canadian_Aboriginal_Syllabics)', 'Balinese', 'Malay_(Jawi_-_Arabic)', 'Early_Aramaic', 'Korean', 'Japanese_(hiragana)', 'Armenian', 'Cyrillic', 'Hebrew', 'Syriac_(Estrangelo)', 'Japanese_(katakana)', 'Blackfoot_(Canadian_Aboriginal_Syllabics)', 'N_Ko', 'Alphabet_of_the_Magi', 'Inuktitut_(Canadian_Aboriginal_Syllabics)', 'Greek', 'Bengali', 'Tagalog', 'Futurama', 'Arcadian', 'Gujarati', 'Asomtavruli_(Georgian)', 'Sanskrit']\n",
            "test alphabets:\n",
            "['ULOG', 'Atemayar_Qelisayer', 'Ge_ez', 'Gurmukhi', 'Tengwar', 'Keble', 'Malayalam', 'Oriya', 'Kannada', 'Mongolian', 'Angelic', 'Atlantean', 'Syriac_(Serto)', 'Aurek-Besh', 'Avesta', 'Glagolitic', 'Sylheti', 'Tibetan', 'Manipuri', 'Old_Church_Slavonic_(Cyrillic)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QP79HYQrXD2k"
      },
      "source": [
        "### Task 2.1: Build  the triplet network (3pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1WHpL8iHAuOH"
      },
      "source": [
        "We will define a triplet Network for use with the Omniglot dataset. Each branch of the triplet  is a \"convnet\" model that transforms data to an embeddings space. \n",
        "\n",
        "*HINT: you may need \"Concatenate\" from keras.layer to merge the output layer*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GQNaMa8hXD2l",
        "colab": {}
      },
      "source": [
        "# define a convnet model to transform data to an embeddings space. \n",
        "# === COMPLETE CODE BELOW ===\n",
        "input_shape = (105, 105, 1)\n",
        "\n",
        "convnet = Sequential(name=\"convnet\")\n",
        "convnet.add(Conv2D(64, (10,10), activation='relu', \n",
        "                   input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
        "convnet.add(MaxPooling2D())\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(Dropout(0.25))\n",
        "convnet.add(Conv2D(128, (7,7), activation='relu', kernel_regularizer=l2(2e-4)))\n",
        "convnet.add(MaxPooling2D())\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(Dropout(0.25))\n",
        "convnet.add(Conv2D(128, (4,4), activation='relu', kernel_regularizer=l2(2e-4)))\n",
        "convnet.add(MaxPooling2D())\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(Dropout(0.25))\n",
        "convnet.add(Conv2D(256, (4,4), activation='relu', kernel_regularizer=l2(2e-4)))\n",
        "convnet.add(Flatten())\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(Dropout(0.25))\n",
        "convnet.add(Dense(4096, activation=\"sigmoid\", kernel_regularizer=l2(1e-3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RXVuWeCsAuOI",
        "outputId": "ee544e77-88e4-4b83-bdce-beef121c0473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "# define a Triplet network.\n",
        "\n",
        "# The anchor, positive, negative image are merged together, as the input \n",
        "# of the triplet network, then got split to get each one's neural codes.\n",
        "generated = Input(shape=(3,105, 105, 1), name='input')\n",
        "\n",
        "anchor  = Lambda(lambda x: x[:,0])(generated)\n",
        "pos     = Lambda(lambda x: x[:,1])(generated)\n",
        "neg     = Lambda(lambda x: x[:,2])(generated)\n",
        "                    \n",
        "anchor_embedding    = convnet(anchor)\n",
        "pos_embedding       = convnet(pos)\n",
        "neg_embedding       = convnet(neg)  \n",
        "\n",
        "# merge the anchor, positive, negative embedding together, \n",
        "# let the merged layer be the output of triplet network\n",
        "\n",
        "# === COMPLETE CODE BELOW ===\n",
        "merged_output = Concatenate(name=\"concatenate\")([anchor_embedding, \n",
        "                                                 pos_embedding, neg_embedding])\n",
        "\n",
        "triplet_net = Model(inputs=generated, outputs=merged_output)\n",
        "triplet_net.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, 3, 105, 105, 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 105, 105, 1)  0           input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 105, 105, 1)  0           input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 105, 105, 1)  0           input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "convnet (Sequential)            (None, 4096)         38985792    lambda[0][0]                     \n",
            "                                                                 lambda_1[0][0]                   \n",
            "                                                                 lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 12288)        0           convnet[1][0]                    \n",
            "                                                                 convnet[2][0]                    \n",
            "                                                                 convnet[3][0]                    \n",
            "==================================================================================================\n",
            "Total params: 38,985,792\n",
            "Trainable params: 38,966,720\n",
            "Non-trainable params: 19,072\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTF32aXqJwle",
        "colab_type": "code",
        "outputId": "e993ef7b-41af-49c9-84d8-81566ee572bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(triplet_net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A-XyrIANAuOM"
      },
      "source": [
        "### Task 2.2: Define triplet loss (2pt)\n",
        "\n",
        "You can find the formula of the triplet loss function in our lecture note. When training our model, make sure the network achieves a smaller loss than the margin and the network does not collapse all representations to zero vectors. \n",
        "\n",
        "*HINT: If you experience problems to achieve this goal, it might be helpful to tinker the learning rate, you can also play with the margin value to get better performance*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mZ3v2Z0RAuON",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Notice that the ground truth variable is not used for loss calculation. \n",
        "# It is used as a function argument to by-pass some Keras functionality.\n",
        "# This is because the network structure already implies the ground truth \n",
        "# for the anchor image with the \"positive\" image.\n",
        "def triplet_loss(ground_truth, network_output):\n",
        "    anchor, positive, negative = tf.split(network_output, \n",
        "                                          num_or_size_splits=3, axis=1)\n",
        "    # === COMPLETE CODE BELOW ===\n",
        "    margin = 0.2\n",
        "    # Subtract two embeddings, square the resulting vector\n",
        "    # and add up all values to form a scalar.\n",
        "    d_pos = tf.reduce_sum(tf.square(anchor - positive), 1)\n",
        "    d_neg = tf.reduce_sum(tf.square(anchor - negative), 1)\n",
        "    loss = tf.reduce_mean(tf.maximum(d_pos - d_neg + margin, 0.0))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DPYHk2styA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "triplet_net.compile(loss=triplet_loss, optimizer=\"adam\")\n",
        "unlearned_params = triplet_net.get_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H7Wo8uzTXD2v"
      },
      "source": [
        "### Task 2.3: Select triplets for training (3pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tSSr9IzTAuOX"
      },
      "source": [
        "#### Different  selection method\n",
        "\n",
        "We have two different options for the triplet selection method, and we will compare the model performance under these two methods after building our model.\n",
        "\n",
        "(1) Random  triplets selection, including the following steps:\n",
        "* Pick one random class for anchor\n",
        "* Pick two different random picture for this class, as the anchor and positive images\n",
        "* Pick another class for Negative, different from anchor_class\n",
        "* Pick one random picture from the negative class.\n",
        "\n",
        "(2) Hard triplets selection. For easy implement, for a picked anchor, positive pair, we will choose the hardest negative to form a hard triplet, that means, after picking an anchor, positive image, we will choose the negative image which is nearest from anchor image from a negative class, ie: \"- d(a,n)\"  can get the maximum value. The whole process including the following steps:\n",
        "* Pick one random class for anchor\n",
        "* Pick two different random picture for this class, as an anchor and positive images\n",
        "* Pick another class for negative, different from anchor_class\n",
        "* Pick one hardest picture from the negative class.\n",
        "\n",
        "*HINT: when picking the hardest negative, you may need the model.predict to get the embedding of images, the calculate the distances*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPDSZEqJ6Cz1",
        "colab_type": "code",
        "outputId": "b5d1bde6-6009-472a-f0cb-dec6426ba64c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(triplet_net.get_layer(\"convnet\").get_input_at(0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"conv2d_input:0\", shape=(None, 105, 105, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn1Dv7fCIZxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding_model(triplet_net):\n",
        "    embedding_model = Model(inputs=triplet_net.get_layer(\"convnet\")\n",
        "    .get_input_at(0), outputs=triplet_net.get_layer(\"convnet\").get_output_at(0))\n",
        "    return embedding_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1G1d26piq1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_negative_class(n_classes, anchor_class):\n",
        "    # Get random negative class.\n",
        "    negative_class = np.random.randint(0, n_classes)\n",
        "    # Get a new random negative class if the same class as the anchor is chosen.\n",
        "    while negative_class == anchor_class:\n",
        "        negative_class = np.random.randint(0, n_classes)\n",
        "    \n",
        "    return negative_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HcxMsmACAuOY",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# Notice that the returned  1 * np.zeros(batch_size) is to by-pass some Keras \n",
        "# functionality, corresponding to ground_truth in tripletloss\n",
        "# We use a variable hard_selection to control which method we are going to use. \n",
        "# If we set hard_selection == False, we will select triplets random. \n",
        "# If we set the variable hard_selection == True, we will select hard triplets.\n",
        "\n",
        "# === COMPLETE CODE BELOW === \n",
        "def get_batch(X, batch_size=64, hard_selection=False, model=None):\n",
        "\n",
        "    if hard_selection:\n",
        "        embedding_model = get_embedding_model(model)\n",
        "\n",
        "    while True:\n",
        "        n_classes, n_examples, w, h = X.shape\n",
        "        # initialize result\n",
        "        triplets=[]\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            triplet = [[],[],[]]\n",
        "            #Pick one random class for anchor\n",
        "            anchor_class = np.random.randint(0, n_classes)\n",
        "\n",
        "            #Pick two different random pics for this class => idx_A and idx_P\n",
        "            [idx_A, idx_P] = np.random.choice(n_examples, size=2, replace=False)\n",
        "\n",
        "            #Pick another class for negative, different from anchor_class\n",
        "            # === COMPLETE CODE BELOW === \n",
        "            negative_class = get_negative_class(n_classes, anchor_class)\n",
        "\n",
        "            if not hard_selection:\n",
        "                #Pick a random pic from this negative class => N\n",
        "                # === COMPLETE CODE BELOW ===   \n",
        "                idx_N = np.random.choice(n_examples, size=1)\n",
        "            else:\n",
        "                #Pick a hardest pic from this negative class => N\n",
        "                # === COMPLETE CODE BELOW ===   \n",
        "                anchor_embedding = embedding_model.predict(\n",
        "                    X[anchor_class, idx_A].reshape(1, w, h, 1))\n",
        "                negative_embedding = embedding_model.predict(\n",
        "                    X[negative_class].reshape(n_examples, w, h, 1))\n",
        "\n",
        "                dist = euclidean_distances(anchor_embedding, negative_embedding)\n",
        "                idx_N = np.argmin(dist)\n",
        "\n",
        "            triplet[0] = X[anchor_class][idx_A].reshape(w, h, 1)\n",
        "            triplet[1] = X[anchor_class][idx_P].reshape(w, h, 1)\n",
        "            triplet[2]=  X[negative_class][idx_N].reshape(w, h, 1)\n",
        "            triplets.append(triplet)\n",
        "\n",
        "        yield np.array(triplets), 1 * np.zeros(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LP1oojLhXD2z"
      },
      "source": [
        "### Task 2.4: One-shot learning with different selection method (2pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XHGJp45AR1qm"
      },
      "source": [
        "Function \"make_oneshot_task\" that can randomly setup such a one-shot task from a given test set (if a language is specified, using only classes/characters from that language), i.e. it will generate N pairs of images, where the first image is always the test image, and the second image is one of the N reference images. The pair of images from the same class will have target 1, all other targets are 0.\n",
        "\n",
        "The function \"test_oneshot\" will generate a number (k) of such one-shot tasks and evaluate the performance of a given model on these tasks; it reports the percentage of correctly classified test images\n",
        "\n",
        "In \"test_oneshot\", you can use embeddings extracted from the triplet network with L2-distance to evaluate one-shot learning. i.e. for a given one-shot task, obtain embeddings for the test image as well as the support set. Then pick the image from the support set that is closest (in L2-distance) to the test image as your one-shot prediction.\n",
        "\n",
        "*HINT you can re-use some code from practice 4b.4*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RBgoMDwMAuOh",
        "colab": {}
      },
      "source": [
        "def make_oneshot_task(N, X, c, language=None):\n",
        "    \"\"\"Create pairs of (test image, support set image) with ground truth, \n",
        "    for testing N-way one-shot learning.\"\"\"\n",
        "    n_classes, n_examples, w, h = X.shape\n",
        "    indices = np.random.randint(0, n_examples, size=(N,))\n",
        "    if language is not None:\n",
        "        low, high = c[language]\n",
        "        if N > high - low:\n",
        "            raise ValueError(\"This language ({}) has less than {} letters\"\n",
        "            .format(language, N))\n",
        "        categories = np.random.choice(range(low,high), size=(N,), replace=False)\n",
        "    else:  # if no language specified just pick a bunch of random letters\n",
        "        categories = np.random.choice(range(n_classes), size=(N,), replace=False)            \n",
        "    true_category = categories[0]\n",
        "    ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n",
        "    test_image = np.asarray([X[true_category, ex1, :, :]]*N).reshape(N, w, h, 1)\n",
        "    support_set = X[categories, indices, :, :]\n",
        "    support_set[0, :, :] = X[true_category, ex2]\n",
        "    support_set = support_set.reshape(N, w, h, 1)\n",
        "    targets = np.zeros((N,))\n",
        "    targets[0] = 1\n",
        "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
        "    pairs = [test_image, support_set]\n",
        "    return pairs, targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rk5F3ffuAuOl",
        "colab": {}
      },
      "source": [
        "def test_oneshot(model, X, c, N=20, k=250, language=None):\n",
        "    # === COMPLETE CODE BELOW ===\n",
        "    n_correct = 0   \n",
        "\n",
        "    embedding_model = get_embedding_model(model)\n",
        "\n",
        "    for i in range(k):\n",
        "        inputs, targets = make_oneshot_task(N, X, c, language=language)\n",
        "\n",
        "        # All values of embeds are the same since inputs[0] contains the same\n",
        "        # picture N times.\n",
        "        embeds = embedding_model.predict(inputs[0])\n",
        "        preds = embedding_model.predict(inputs[1])\n",
        "        # Calculate the distance between the reference image's embedding and the\n",
        "        # support set image's embeddings. \n",
        "        distances = [tf.norm(x - y) for x, y in zip(embeds, preds)]\n",
        "        if tf.argmin(distances) == tf.argmax(targets):\n",
        "            n_correct += 1\n",
        "\n",
        "    return (100 * n_correct / k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u6yMu4jlXD26"
      },
      "source": [
        "With different triplets selecting method (random and hard), we will train our model and evaluate the model by one-shot learning accuracy.\n",
        "\n",
        "* You need to explicitly state the accuracy under different  triplets selecting method\n",
        "* When evaluating model with test_oneshot function, you should evaluate on 20 way one-shot task, and set the number (k) of evaluation one-shot tasks to be 250, then calculate the average accuracy\n",
        "\n",
        "*HINT: After training our model with random selection method, before train model under hard triplets selection, we should re-build our model (re-run the cell in Task 2.1) to initialize our model and prevent re-use the trained model of random selection*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "snKKGhIoXD27"
      },
      "source": [
        "#### Evaluate one-shot learning with  random triplets selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRabX5PTscs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, X_train, batch_size=64, steps_per_epoch=100, epochs=1,\n",
        "          hard_selection=False):\n",
        "    model.fit(get_batch(X_train, batch_size, hard_selection=hard_selection,\n",
        "                        model=model), \n",
        "              steps_per_epoch=steps_per_epoch, epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e-r2qB5dAuOt",
        "outputId": "7d5b740f-5354-4267-ee46-ff4493276f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "source": [
        "# hard_selection == False, select triplets randomly\n",
        "# Train our model and evaluate the model by one-shot learning accuracy.\n",
        "loops = 10\n",
        "best_acc = 0\n",
        "\n",
        "for i in range(loops):\n",
        "    print(\"=== Training loop {} ===\".format(i+1))\n",
        "    # === ADD CODE HERE ===\n",
        "    train(triplet_net, X_train, hard_selection=False)\n",
        "    test_acc = test_oneshot(triplet_net, X_test, c_test)\n",
        "    print(\"Current accuracy: {}\".format(test_acc))\n",
        "    if test_acc >= best_acc:\n",
        "        best_acc = test_acc\n",
        "        print(\"New best accuracy: {}\".format(best_acc))\n",
        "\n",
        "print(\"Final best accuracy: {}\".format(best_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Training loop 1 ===\n",
            "100/100 [==============================] - 36s 359ms/step - loss: 24.2854\n",
            "Current accuracy: 48.0\n",
            "New best accuracy: 48.0\n",
            "=== Training loop 2 ===\n",
            "100/100 [==============================] - 36s 361ms/step - loss: 23.4234\n",
            "Current accuracy: 58.0\n",
            "New best accuracy: 58.0\n",
            "=== Training loop 3 ===\n",
            "100/100 [==============================] - 36s 356ms/step - loss: 22.3636\n",
            "Current accuracy: 62.0\n",
            "New best accuracy: 62.0\n",
            "=== Training loop 4 ===\n",
            "100/100 [==============================] - 36s 355ms/step - loss: 22.3798\n",
            "Current accuracy: 66.4\n",
            "New best accuracy: 66.4\n",
            "=== Training loop 5 ===\n",
            "100/100 [==============================] - 35s 355ms/step - loss: 22.5035\n",
            "Current accuracy: 63.6\n",
            "=== Training loop 6 ===\n",
            "100/100 [==============================] - 35s 355ms/step - loss: 22.4180\n",
            "Current accuracy: 64.0\n",
            "=== Training loop 7 ===\n",
            "100/100 [==============================] - 35s 354ms/step - loss: 21.5739\n",
            "Current accuracy: 62.4\n",
            "=== Training loop 8 ===\n",
            "100/100 [==============================] - 35s 355ms/step - loss: 22.5077\n",
            "Current accuracy: 65.2\n",
            "=== Training loop 9 ===\n",
            "100/100 [==============================] - 35s 354ms/step - loss: 21.4147\n",
            "Current accuracy: 68.4\n",
            "New best accuracy: 68.4\n",
            "=== Training loop 10 ===\n",
            "100/100 [==============================] - 35s 354ms/step - loss: 21.7836\n",
            "Current accuracy: 68.0\n",
            "Final best accuracy: 68.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYeEJTGtKwAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset the weights\n",
        "triplet_net.set_weights(unlearned_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YLlvr6TRXD2-"
      },
      "source": [
        "#### Evaluate one-shot learning with  hard triplets selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CN_IDkRHqjb5",
        "outputId": "5712e5ba-f86a-4832-ac52-75af6655df35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "# hard_selection == True, select hard triplets\n",
        "# Train our model and evaluate the model by one-shot learning accuracy.\n",
        "loops = 10\n",
        "best_acc = 0\n",
        "\n",
        "for i in range(loops):\n",
        "    print(\"=== Training loop {} ===\".format(i+1))\n",
        "    # === ADD CODE HERE ===\n",
        "    train(triplet_net, X_train, hard_selection=True)\n",
        "    test_acc = test_oneshot(triplet_net, X_test, c_test)\n",
        "    print(\"Current accuracy: {}\".format(test_acc))\n",
        "    if test_acc >= best_acc:\n",
        "        best_acc = test_acc\n",
        "        print(\"New best accuracy: {}\".format(best_acc))\n",
        "\n",
        "print(\"Final best accuracy: {}\".format(best_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Training loop 1 ===\n",
            "100/100 [==============================] - 655s 7s/step - loss: 45.7904\n",
            "Current accuracy: 62.8\n",
            "New best accuracy: 62.8\n",
            "=== Training loop 2 ===\n",
            "100/100 [==============================] - 658s 7s/step - loss: 42.1176\n",
            "Current accuracy: 71.2\n",
            "New best accuracy: 71.2\n",
            "=== Training loop 3 ===\n",
            "100/100 [==============================] - 664s 7s/step - loss: 37.7094\n",
            "Current accuracy: 73.6\n",
            "New best accuracy: 73.6\n",
            "=== Training loop 4 ===\n",
            "100/100 [==============================] - 650s 7s/step - loss: 37.5147\n",
            "Current accuracy: 72.4\n",
            "=== Training loop 5 ===\n",
            "100/100 [==============================] - 648s 6s/step - loss: 35.2829\n",
            "Current accuracy: 72.0\n",
            "=== Training loop 6 ===\n",
            "100/100 [==============================] - 647s 6s/step - loss: 33.3048\n",
            "Current accuracy: 70.0\n",
            "=== Training loop 7 ===\n",
            "100/100 [==============================] - 681s 7s/step - loss: 33.2560\n",
            "Current accuracy: 74.0\n",
            "New best accuracy: 74.0\n",
            "=== Training loop 8 ===\n",
            "100/100 [==============================] - 679s 7s/step - loss: 32.2187\n",
            "Current accuracy: 78.4\n",
            "New best accuracy: 78.4\n",
            "=== Training loop 9 ===\n",
            "100/100 [==============================] - 673s 7s/step - loss: 31.7669\n",
            "Current accuracy: 75.6\n",
            "=== Training loop 10 ===\n",
            "100/100 [==============================] - 678s 7s/step - loss: 31.7224\n",
            "Current accuracy: 78.8\n",
            "New best accuracy: 78.8\n",
            "Final best accuracy: 78.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4aWNzDOBVyM",
        "colab_type": "text"
      },
      "source": [
        "# Task 3 - Peer review (0 pt):\n",
        "Finally, each group member must write a single paragraph outlining their opinion on the work distribution within the group. Did every group member\n",
        "contribute equally? Did you split up tasks in a fair manner, or jointly worked through the exercises. Do you think that some members of your group deserve a different grade from others? You can use the table below to make an overview of how the tasks were divided:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjhpkFhyBXs8",
        "colab_type": "text"
      },
      "source": [
        "__Luc__: Gerrit did assignment 2.1 and I did assignment 2.2. I think we split the work fairly. We both evaluated eachother's work and improved where needed. For these reasons I think we deserve the same grade.\n",
        "\n",
        "__Gerrit__: We distributed the tasks evenly. Luc worked on Question 2 and I worked on Question 1. Once, we were both done with our parts we shared the notebooks with each other and discussed the results. Hence, the workload was split up in a fair manner and we both deserve the same grade."
      ]
    }
  ]
}